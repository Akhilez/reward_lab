{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nine_mens_morris import nmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = nmm.env(render_mode='ansi')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.reset()\n",
    "print(env.possible_agents)\n",
    "print(env.action_spaces['player_0'], env.observation_spaces['player_0'])\n",
    "obs = env.observe('player_0')\n",
    "obs['observation'].shape, obs['action_mask']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import MeanMetric\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import one_hot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AlphaZeroNMM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 76 = (24 pieces of current player) + (24 pieces of opponent) + (24, position of the lifted piece to move) + (4 action types: place_phase_1, lift_phase_2, drop_phase_2, kill)\n",
    "        self.v = nn.Sequential(\n",
    "            nn.Linear(76, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.p_place = nn.Sequential(\n",
    "            nn.Linear(76, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 24),\n",
    "        )\n",
    "\n",
    "        self.p_lift = nn.Sequential(\n",
    "            nn.Linear(76, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 24),\n",
    "        )\n",
    "\n",
    "        self.p_drop = nn.Sequential(\n",
    "            nn.Linear(76, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 24),\n",
    "        )\n",
    "\n",
    "        self.p_kill = nn.Sequential(\n",
    "            nn.Linear(76, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 24),\n",
    "        )\n",
    "\n",
    "    def forward(self, s, action_types):\n",
    "        # Add action types to observation because the value network should know which phase you're in to make it fully observable.\n",
    "        action_type_one_hot = one_hot(action_types, num_classes=4)  # (b, 4)\n",
    "        s = torch.cat((s, action_type_one_hot), dim=1)\n",
    "\n",
    "        v = self.v(s)  # (b, 1)\n",
    "        p_place = self.p_place(s)  # (b, 24)\n",
    "        p_lift = self.p_lift(s)  # (b, 24)\n",
    "        p_drop = self.p_drop(s)  # (b, 24)\n",
    "        p_kill = self.p_kill(s)  # (b, 24)\n",
    "\n",
    "        p = []\n",
    "        for i in range(len(s)):\n",
    "            if action_types[i] == nmm.ActionType.PLACE:\n",
    "                p.append(p_place[i])\n",
    "            elif action_types[i] == nmm.ActionType.LIFT:\n",
    "                p.append(p_lift[i])\n",
    "            elif action_types[i] == nmm.ActionType.DROP:\n",
    "                p.append(p_drop[i])\n",
    "            elif action_types[i] == nmm.ActionType.KILL:\n",
    "                p.append(p_kill[i])\n",
    "            else:\n",
    "                raise Exception(f'Action type must be 0, 1, 2 or 3.')\n",
    "        p = torch.stack(p)  # (b, 24)\n",
    "        p = torch.softmax(p, dim=1)\n",
    "\n",
    "        return p, v"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MCTS\n",
    "\n",
    "Similar to AlphaZero's MCTS, but with following changes:\n",
    "\n",
    "1. Agents do not necessarily switch turns at every level.\n",
    "2. Limited number of visits for each node in case of cyclic state graph (loops).\n",
    "    https://github.com/jonathan-laurent/AlphaZero.jl/issues/47\n",
    "   a. This is taken care by the limit to the number of steps in the env."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.q_sa = {}  # stores Q values for (s, a)\n",
    "        self.n_sa = {}  # stores # of times edge (s, a) was visited\n",
    "        self.p_s = {}  # stores initial policy returned by neural net\n",
    "\n",
    "    def simulate(self, env):\n",
    "        s = env.state()\n",
    "        obs = env.observe(env.agent_selection)\n",
    "        action_mask = obs['action_mask']\n",
    "        action_type = obs['action_type']\n",
    "        obs = obs['observation']\n",
    "\n",
    "        \"\"\" EVALUATE \"\"\"\n",
    "        # If s is leaf (different from terminal state), then evaluate s with model\n",
    "        if s not in self.p_s:\n",
    "            p, v = self.evaluate(obs, action_mask, action_type)\n",
    "            self.p_s[s] = p\n",
    "            return v  # This is NOT -v because sign doesn't necessarily change at every level.\n",
    "\n",
    "        \"\"\" SELECT \"\"\"\n",
    "        cpuct = 1\n",
    "        q = np.array([self.q_sa.get((s, a), 0) for a in range(len(action_mask))])\n",
    "        n = np.array([self.n_sa.get((s, a), 0) for a in range(len(action_mask))])\n",
    "\n",
    "        # (1− ε)pa+ εηa, where η∼ Dir(0.03) and ε= 0.25\n",
    "        p = 0.75 * self.p_s[s] + 0.25 * np.random.dirichlet([2] * len(q))\n",
    "\n",
    "        u = q + cpuct * p * np.sqrt(n.sum() + 1e-6) / (1 + n)\n",
    "        u[action_mask == 0] = -np.inf\n",
    "        a = u.argmax()\n",
    "\n",
    "        \"\"\" EXPAND \"\"\"\n",
    "        next_env = deepcopy(env)\n",
    "        next_env.step(a)\n",
    "        agent, next_agent = env.agent_selection, next_env.agent_selection\n",
    "        # If terminal state, v = z\n",
    "        if next_env.terminations[agent] or next_env.truncations[next_agent]:\n",
    "            v = next_env.rewards[agent]\n",
    "        else:\n",
    "            v = self.simulate(next_env)\n",
    "            if agent != next_agent:\n",
    "                v = -v  # Change sign only if next agent is opponent.\n",
    "\n",
    "        \"\"\" BACKTRACK \"\"\"\n",
    "        if (s, a) in self.q_sa:\n",
    "            # Recompute the average\n",
    "            sum_ = self.n_sa[(s, a)] * self.q_sa[(s, a)] + v\n",
    "            self.n_sa[(s, a)] += 1\n",
    "            self.q_sa[(s, a)] = sum_ / self.n_sa[(s, a)]\n",
    "        else:\n",
    "            self.q_sa[(s, a)] = v\n",
    "            self.n_sa[(s, a)] = 1\n",
    "        return v  # Again, this is NOT -v.\n",
    "\n",
    "    def evaluate(self, obs, action_mask, action_type):\n",
    "        obs = torch.from_numpy(obs).float().view(1, -1)  # (1, 48)\n",
    "\n",
    "        # Get predictions\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            p, v = self.model(obs.float().to(device), torch.LongTensor([action_type]).to(device))  # (1, 24), (1, 1)\n",
    "        p = p.cpu().numpy()[0]  # (24,)\n",
    "        v = v.cpu().numpy()[0][0]  # scalar\n",
    "\n",
    "        # Legal action masking.\n",
    "        p *= action_mask\n",
    "\n",
    "        # Normalizing probabilities\n",
    "        p_sum = p.sum()\n",
    "        if p_sum > 0:\n",
    "            p /= p_sum  # re-normalize\n",
    "        else:\n",
    "            print(\"All valid moves are masked, doing a workaround.\")\n",
    "            p = action_mask / action_mask.sum()\n",
    "\n",
    "        return p, v\n",
    "\n",
    "    def get_action_prob(self, env, temperature=1):\n",
    "        s = env.state()\n",
    "        p = [self.n_sa.get((s, a), 0) for a in range(len(self.p_s[s]))]\n",
    "        p = np.array(p, dtype=np.float32) ** 1 / temperature\n",
    "        p *= env.action_masks[env.agent_selection]\n",
    "        return p / p.sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot(iteration, losses, win_rates, lose_rates):\n",
    "    clear_output(True)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    ax1.set_title(f\"Loss. Iteration: {iteration}\")\n",
    "    ax1.plot(losses)\n",
    "    ax2.set_title(f'Win rate and lose rate')\n",
    "    ax2.plot(win_rates, label='Win Rate')\n",
    "    ax2.plot(lose_rates, label='Lose Rate')\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate(env, model1, model2, n_episodes):\n",
    "    wins1, wins2 = 0, 0\n",
    "    for _ in tqdm(range(n_episodes), desc='Evaluating'):\n",
    "        env.reset()\n",
    "        mcts_ = MCTS(model1), MCTS(model2)\n",
    "        step = 0\n",
    "        while not (env.terminations[env.agent_selection] or env.truncations[env.agent_selection]):\n",
    "            step += 1\n",
    "            mcts = mcts_[int(step % 2 == 0)]\n",
    "\n",
    "            [mcts.simulate(env) for _ in range(10)]\n",
    "            p = mcts.get_action_prob(env)\n",
    "            action = np.random.choice(len(p), p=p)\n",
    "\n",
    "            env.step(action)\n",
    "        r = env.rewards[env.agents[0]]\n",
    "        wins1 += int(r > 1)\n",
    "        wins2 += int(r < 1)\n",
    "    return wins1 / n_episodes, wins2 / n_episodes\n",
    "\n",
    "\n",
    "def self_play(env, mcts: MCTS):\n",
    "    tuples = []\n",
    "    env.reset()\n",
    "    while not (env.terminations[env.agent_selection] or env.truncations[env.agent_selection]):\n",
    "        [mcts.simulate(env) for _ in range(100)]\n",
    "        p = mcts.get_action_prob(env)\n",
    "        action = np.random.choice(len(p), p=p)\n",
    "        obs = env.observe(env.agent_selection)\n",
    "        tuples.append([obs['observation'], obs['action_type'], p, env.agent_selection])\n",
    "\n",
    "        env.step(action)\n",
    "    for tup in tuples:\n",
    "        tup[-1] = env.rewards[tup[-1]]\n",
    "    return tuples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = nmm.env()\n",
    "\n",
    "device = 'cuda:1'  # 'cpu'\n",
    "model = AlphaZeroNMM().to(device)\n",
    "old_model = deepcopy(model)\n",
    "optimizer = Adam(model.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "\n",
    "losses = []\n",
    "win_rates = []\n",
    "lose_rates = []\n",
    "dataset = deque([], maxlen=50000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for iteration in range(500):\n",
    "    \"\"\" Self Play \"\"\"\n",
    "    for _ in tqdm(range(25), desc='Self-Play'):\n",
    "        mcts = MCTS(model)\n",
    "        dataset += self_play(env, mcts)\n",
    "\n",
    "    \"\"\" Train \"\"\"\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(10), desc='Training'):\n",
    "        loss_mean = MeanMetric()\n",
    "        for state, action_type, p, v in DataLoader(dataset, batch_size=16, shuffle=True):\n",
    "            p_pred, v_pred = model(state.float().to(device), torch.LongTensor(action_type).to(device))\n",
    "            loss = (v.to(device) - v_pred).pow(2).mean() - (p.to(device) * p_pred.log()).mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_mean(loss.item())\n",
    "        losses.append(loss_mean.compute())\n",
    "\n",
    "    \"\"\" Evaluate \"\"\"\n",
    "    wins_0, loses_0 = evaluate(env, model, old_model, 10)\n",
    "    loses_1, wins_1 = evaluate(env, old_model, model, 10)\n",
    "\n",
    "    win_rates.append((wins_0 + wins_1) / 2)\n",
    "    lose_rates.append((loses_0 + loses_1) / 2)\n",
    "\n",
    "    # if iteration % 5 == 0:\n",
    "    #     old_model = deepcopy(model)\n",
    "    if win_rates[-1] > lose_rates[-1]:\n",
    "        old_model = deepcopy(model)\n",
    "    # else:\n",
    "    #     model = deepcopy(old_model)\n",
    "\n",
    "    plot(iteration, losses, win_rates, lose_rates)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/Users/akhildevarashetti/code/reward_lab/exp/nmm/weights/model_3.pth')\n",
    "torch.save(model.state_dict(), '/home/akhil/code/reward_lab/exp/nmm/weights/model_2.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample Gameplay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env.reset()\n",
    "mcts = MCTS(model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[mcts.simulate(env) for _ in range(25)]\n",
    "p = mcts.get_action_prob(env)\n",
    "# action = np.random.choice(len(p), p=p)\n",
    "action = p.argmax()\n",
    "for i, pi in enumerate(p):\n",
    "    print(f'{i}:\\t{pi}')\n",
    "print(f'{action=}\\n{env.agent_selection=}\\n{env.get_action_type(env.agent_selection)}')\n",
    "\n",
    "print(env.render())\n",
    "env.step(action)\n",
    "print(f'{env.rewards=}')\n",
    "print(env.render())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
